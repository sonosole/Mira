## probabilistic loss

export crossEntropy
export crossEntropyLoss

export binaryCrossEntropy
export binaryCrossEntropyLoss

export focalBCE
export focalBCELoss


"""
    crossEntropy(p::Variable{T}, ğœŒ::Variable{T}) -> y::Variable{T}
cross entropy is `y = - ğœŒ * log(p) where ğœŒ is the target and p is the output of the network.
"""
function crossEntropy(p::Variable{T}, ğœŒ::Variable{T}) where T
    @assert (p.shape == ğœŒ.shape)
    backprop = (p.backprop || ğœŒ.backprop)
    Ïµ = eltype(p)(1e-38)
    y = Variable{T}(- áµ›(ğœŒ) .* log.(áµ›(p) .+ Ïµ), backprop)
    if backprop
        y.backward = function crossEntropyBackward()
            if need2computeÎ´!(p)
                Î´(p) .-= Î´(y) .* áµ›(ğœŒ) ./ (áµ›(p) .+ Ïµ)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


"""
    crossEntropy(p::Variable{T}, ğœŒ::AbstractArray) -> y::Variable{T}
cross entropy is `y = - ğœŒ * log(p) where ğœŒ is the target and p is the output of the network.
"""
function crossEntropy(p::Variable{T}, ğœŒ::AbstractArray) where T
    @assert p.shape == size(ğœŒ)
    Ïµ = eltype(p)(1e-38)
    y = Variable{T}(- ğœŒ .* log.(áµ›(p) .+ Ïµ), p.backprop)
    if y.backprop
        y.backward = function crossEntropyBackward()
            if need2computeÎ´!(p)
                Î´(p) .-= Î´(y) .* ğœŒ ./ (áµ›(p) .+ Ïµ)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


"""
    binaryCrossEntropy(p::Variable{T}, ğœŒ::Variable{T}) -> y::Variable{T}
binary cross entropy is `y = - ğœŒlog(p) - (1-ğœŒ)log(1-p)` where ğœŒ is the target and p is the output of the network.
"""
function binaryCrossEntropy(p::Variable{T}, ğœŒ::Variable{T}) where T
    @assert (p.shape == ğœŒ.shape)
    backprop = (p.backprop || ğœŒ.backprop)
    TOO  = eltype(p)
    Ïµ  = TOO(1e-38)
    ğŸ™  = TOO(1.0f0)
    tâ‚ = -       áµ›(ğœŒ)  .* log.(     áµ›(p) .+ Ïµ)
    tâ‚‚ = - (ğŸ™ .- áµ›(ğœŒ)) .* log.(ğŸ™ .- áµ›(p) .+ Ïµ)
    y  = Variable{T}(tâ‚ + tâ‚‚, backprop)
    if backprop
        y.backward = function binaryCrossEntropyBackward()
            if need2computeÎ´!(p)
                Î´â‚ = (ğŸ™ .- áµ›(ğœŒ)) ./ (ğŸ™ .- áµ›(p) .+ Ïµ)
                Î´â‚‚ =       áµ›(ğœŒ)  ./ (     áµ›(p) .+ Ïµ)
                Î´(p) .+= Î´(y) .* (Î´â‚ - Î´â‚‚)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y);
        end
        addchild(y, p)
    end
    return y
end


"""
    binaryCrossEntropy(p::Variable{T}, ğœŒ::AbstractArray) -> y::Variable{T}
binary cross entropy is `y = - ğœŒlog(p) - (1-ğœŒ)log(1-p)` where ğœŒ is the target and p is the output of the network.
"""
function binaryCrossEntropy(p::Variable{T}, ğœŒ::AbstractArray) where T
    @assert p.shape == size(ğœŒ)
    TO = eltype(p)
    Ïµ  = TO(1e-38)
    ğŸ™  = TO(1.0f0)
    tâ‚ = -       ğœŒ  .* log.(     áµ›(p) .+ Ïµ)
    tâ‚‚ = - (ğŸ™ .- ğœŒ) .* log.(ğŸ™ .- áµ›(p) .+ Ïµ)
    y  = Variable{T}(tâ‚ + tâ‚‚, p.backprop)
    if y.backprop
        y.backward = function binaryCrossEntropyBackward()
            if need2computeÎ´!(p)
                Î´â‚ = (ğŸ™ .- ğœŒ) ./ (ğŸ™ .- áµ›(p) .+ Ïµ)
                Î´â‚‚ =       ğœŒ  ./ (     áµ›(p) .+ Ïµ)
                Î´(p) .+= Î´(y) .* (Î´â‚ - Î´â‚‚)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


"""
    binaryCrossEntropy(p::AbstractArray, label::AbstractArray) -> lossvalue::AbstractArray
binary cross entropy is `y = - label*log(p) - (1-label)*log(1-p)` where p is the output of the network.
"""
function binaryCrossEntropy(p::AbstractArray, label::AbstractArray)
    @assert size(p) == size(label)
    TO = eltype(p)
    Ïµ  = TO(1e-38)
    ğŸ™  = TO(1.0f0)
    tâ‚ = -       label  .* log.(     p .+ Ïµ)
    tâ‚‚ = - (ğŸ™ .- label) .* log.(ğŸ™ .- p .+ Ïµ)
    return tâ‚ + tâ‚‚
end


"""
    crossEntropyLoss(p::AbstractArray, label::AbstractArray) -> lossvalue::AbstractArray
cross entropy is `y = - label * log(p) where p is the output of the network.
"""
function crossEntropyLoss(p::AbstractArray, label::AbstractArray)
    @assert size(p) == size(label)
    Ïµ = eltype(p)(1e-38)
    y = - label .* log.(p .+ Ïµ)
    return y
end


crossEntropyLoss(x::Variable{T}, label::Variable{T}; reduction::String="sum") where T = loss( crossEntropy(x, label), reduction=reduction )
crossEntropyLoss(x::Variable{T}, label::AbstractArray; reduction::String="sum") where T = loss( crossEntropy(x, label), reduction=reduction )
crossEntropyLoss(x::AbstractArray, label::AbstractArray; reduction::String="sum") = loss( crossEntropy(x, label), reduction=reduction )

binaryCrossEntropyLoss(x::Variable{T}, label::Variable{T}; reduction::String="sum") where T = loss(binaryCrossEntropy(x, label), reduction=reduction)
binaryCrossEntropyLoss(x::Variable{T}, label::AbstractArray; reduction::String="sum") where T = loss(binaryCrossEntropy(x, label), reduction=reduction)
binaryCrossEntropyLoss(x::AbstractArray, label::AbstractArray; reduction::String="sum") = loss(binaryCrossEntropy(x, label), reduction=reduction)


function focalBCE(p::Variable{T}, ğœŒ::AbstractArray; gamma::Real=2, alpha::Real=0.5) where T
    @assert p.shape == size(ğœŒ)
    TO = eltype(p)
    Ïµ  = TO(1e-38)
    ğŸ™  = TO(1.0f0)
    Î³  = gamma
    Î±  = alpha
    ğ’‘  = áµ›(p)

    wâ‚ = @. -      Î±  *      ğœŒ
    wâ‚‚ = @. - (ğŸ™ - Î±) * (ğŸ™ - ğœŒ)

    tâ‚ = @. wâ‚ * (ğŸ™ - ğ’‘)^ Î³ * log(    ğ’‘ + Ïµ)
    tâ‚‚ = @. wâ‚‚ *      ğ’‘ ^ Î³ * log(ğŸ™ - ğ’‘ + Ïµ)

    y  = Variable{T}(tâ‚ + tâ‚‚, p.backprop)

    if y.backprop
        y.backward = function focalBCEBackward()
            if need2computeÎ´!(p)
                Î´â‚ = @. wâ‚ * (ğŸ™ - ğ’‘)^(Î³-1) * (ğŸ™ / ğ’‘ - Î³ * log(ğ’‘) - ğŸ™)
                Î´â‚‚ = @. wâ‚‚ * ğ’‘ ^ Î³ * (ğŸ™ / (ğ’‘ - ğŸ™) + Î³ * log(ğŸ™ - ğ’‘) / ğ’‘)
                Î´(p) .+= Î´(y) .* (Î´â‚ + Î´â‚‚)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


focalBCELoss(x::Variable{T}, label::AbstractArray; reduction::String="sum") where T = loss(focalBCE(x, label), reduction=reduction)
