## probabilistic loss

export CrossEntropy
export CrossEntropyLoss

export BinaryCrossEntropy
export BinaryCrossEntropyLoss

export FocalCE
export FocalCELoss
export FocalBCE
export FocalBCELoss

export NLogCrossEntropy
export NLogCELoss

export InvPowerCrossEntropy
export InvPowerCELoss


"""
    CrossEntropy(p::Variable{T}, label::Variable{T}) -> y::Variable{T}
cross entropy is `y = - label * log(p)` where `p` is the output of the network.
"""
function CrossEntropy(p::Variable{T}, label::Variable{T}) where T
    @assert (p.shape == label.shape)
    backprop = (p.backprop || label.backprop)
    ğ† = áµ›(label)
    ğ’‘ = áµ›(p) .+ eltype(p)(1e-38)
    y = Variable{T}(- ğ† .* log.(ğ’‘), backprop)
    if backprop
        y.backward = function âˆ‡CrossEntropy()
            if need2computeÎ´!(p)
                Î´(p) .-= Î´(y) .* ğ† ./ ğ’‘
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


"""
    CrossEntropy(p::Variable, label::AbstractArray) -> y::Variable
cross entropy is `y = - label * log(p)` where `p` is the output of the network.
"""
function CrossEntropy(p::Variable{T}, label::AbstractArray) where T
    @assert p.shape == size(label)
    ğ† = label
    ğ’‘ = áµ›(p) .+ eltype(p)(1e-38)
    y = Variable{T}(- ğ† .* log.(ğ’‘), p.backprop)
    if y.backprop
        y.backward = function âˆ‡CrossEntropy()
            if need2computeÎ´!(p)
                Î´(p) .-= Î´(y) .* ğ† ./ ğ’‘
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


"""
    CrossEntropy(p::AbstractArray, label::AbstractArray) -> lossvalue::AbstractArray
cross entropy is `y = - label * log(p)` where `p` is the output of the network.
"""
function CrossEntropy(p::AbstractArray, label::AbstractArray)
    @assert size(p) == size(label)
    Ïµ = eltype(p)(1e-38)
    y = - label .* log.(p .+ Ïµ)
    return y
end


"""
    BinaryCrossEntropy(p::Variable{T}, label::Variable{T}) -> y::Variable{T}
binary cross entropy is `y = - label*log(p) - (1-label)*log(1-p)` where `p` is the output of the network.
"""
function BinaryCrossEntropy(p::Variable{T}, label::Variable{T}) where T
    @assert (p.shape == label.shape)
    backprop = (p.backprop || label.backprop)
    TO = eltype(p)
    Ïµ  = TO(1e-38)
    ğŸ™  = TO(1.0f0)
    ğ†  = áµ›(label)
    ğ’‘  = áµ›(p) .+ Ïµ
    tâ‚ = @. -      ğ†  * log(    ğ’‘)
    tâ‚‚ = @. - (ğŸ™ - ğ†) * log(ğŸ™ - ğ’‘)
    y  = Variable{T}(tâ‚ + tâ‚‚, backprop)
    if backprop
        y.backward = function âˆ‡BinaryCrossEntropy()
            if need2computeÎ´!(p)
                Î´â‚ = @. (ğŸ™ - ğ†) / (ğŸ™ - ğ’‘)
                Î´â‚‚ = @. ğ† / ğ’‘
                Î´(p) .+= Î´(y) .* (Î´â‚ - Î´â‚‚)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


"""
    BinaryCrossEntropy(p::Variable, label::AbstractArray) -> y::Variable
binary cross entropy is `y = - label*log(p) - (1-label)*log(1-p)` where `p` is the output of the network.
"""
function BinaryCrossEntropy(p::Variable{T}, label::AbstractArray) where T
    @assert p.shape == size(label)
    TO = eltype(p)
    Ïµ  = TO(1e-38)
    ğŸ™  = TO(1.0f0)
    ğ†  = label
    ğ’‘  = áµ›(p) .+ Ïµ
    tâ‚ = @. -      ğ†  * log(    ğ’‘)
    tâ‚‚ = @. - (ğŸ™ - ğ†) * log(ğŸ™ - ğ’‘)
    y  = Variable{T}(tâ‚ + tâ‚‚, p.backprop)
    if y.backprop
        y.backward = function âˆ‡BinaryCrossEntropy()
            if need2computeÎ´!(p)
                Î´â‚ = @. (ğŸ™ - ğ†) / (ğŸ™ - ğ’‘)
                Î´â‚‚ = @. ğ† / ğ’‘
                Î´(p) .+= Î´(y) .* (Î´â‚ - Î´â‚‚)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


"""
    BinaryCrossEntropy(p::AbstractArray, label::AbstractArray) -> lossvalue::AbstractArray
binary cross entropy is `y = - label*log(p) - (1-label)*log(1-p)` where `p` is the output of the network.
"""
function BinaryCrossEntropy(p::AbstractArray, label::AbstractArray)
    @assert size(p) == size(label)
    TO = eltype(p)
    Ïµ  = TO(1e-38)
    ğŸ™  = TO(1.0f0)
    ğ’‘  = p + Ïµ
    tâ‚ = @. -      label  * log(    ğ’‘)
    tâ‚‚ = @. - (ğŸ™ - label) * log(ğŸ™ - ğ’‘)
    return tâ‚ + tâ‚‚
end


CrossEntropyLoss(x::Variable{T}, label::Variable{T}; reduction::String="sum") where T = Loss( CrossEntropy(x, label), reduction=reduction )
CrossEntropyLoss(x::Variable{T}, label::AbstractArray; reduction::String="sum") where T = Loss( CrossEntropy(x, label), reduction=reduction )
CrossEntropyLoss(x::AbstractArray, label::AbstractArray; reduction::String="sum") = Loss( CrossEntropy(x, label), reduction=reduction )

BinaryCrossEntropyLoss(x::Variable{T}, label::Variable{T}; reduction::String="sum") where T = Loss(BinaryCrossEntropy(x, label), reduction=reduction)
BinaryCrossEntropyLoss(x::Variable{T}, label::AbstractArray; reduction::String="sum") where T = Loss(BinaryCrossEntropy(x, label), reduction=reduction)
BinaryCrossEntropyLoss(x::AbstractArray, label::AbstractArray; reduction::String="sum") = Loss(BinaryCrossEntropy(x, label), reduction=reduction)


"""
    FocalBCE(p::Variable, label::AbstractArray; focus::Real=1.0f0, alpha::Real=0.5f0)

focal loss version BinaryCrossEntropy
"""
function FocalBCE(p::Variable{T}, label::AbstractArray; focus::Real=1.0f0, alpha::Real=0.5f0) where T
    @assert p.shape == size(label)
    TO = eltype(p)
    Ïµ  = TO(1e-38)
    ğŸ™  = TO(1.0f0)
    Î³  = TO(focus)
    Î±  = TO(alpha)
    ğ†  = label
    ğ’‘  = áµ›(p) .+ Ïµ

    wâ‚ = @. -      Î±  *      ğ†
    wâ‚‚ = @. - (ğŸ™ - Î±) * (ğŸ™ - ğ†)

    tâ‚ = @. wâ‚ * (ğŸ™ - ğ’‘) ^ Î³ * log(    ğ’‘)
    tâ‚‚ = @. wâ‚‚ *      ğ’‘  ^ Î³ * log(ğŸ™ - ğ’‘)

    y  = Variable{T}(tâ‚ + tâ‚‚, p.backprop)

    if y.backprop
        y.backward = function âˆ‡FocalBCE()
            if need2computeÎ´!(p)
                Î´â‚ = @. wâ‚ * (ğŸ™ - ğ’‘)^(Î³ - ğŸ™) * (ğŸ™ / ğ’‘ - Î³ * log(ğ’‘) - ğŸ™)
                Î´â‚‚ = @. wâ‚‚ * ğ’‘ ^ Î³ * (ğŸ™ / (ğ’‘ - ğŸ™) + Î³ * log(ğŸ™ - ğ’‘) / ğ’‘)
                Î´(p) .+= Î´(y) .* (Î´â‚ + Î´â‚‚)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


"""
    FocalCE(p::Variable, label::AbstractArray; focus::Real=1.0f0)

focal loss version CrossEntropy
"""
function FocalCE(p::Variable{T}, label::AbstractArray; focus::Real=1.0f0) where T
    @assert p.shape == size(label)
    TO = eltype(p)
    Ïµ  = TO(1e-38)
    ğŸ™  = TO(1.0f0)
    Î³  = TO(focus)
    ğ’‘  = áµ›(p) .+ Ïµ
    ğ†  = label

    t = @. - ğ† * (ğŸ™ - ğ’‘) ^ Î³ * log(ğ’‘)
    y = Variable{T}(t, p.backprop)

    if y.backprop
        y.backward = function âˆ‡FocalCE()
            if need2computeÎ´!(p)
                Î´p = Î´(p)
                Î´y = Î´(y)
                @. Î´p += Î´y * ğ† * (ğŸ™ - ğ’‘)^(Î³ - ğŸ™) * (Î³ * log(ğ’‘) + ğŸ™ - ğŸ™ / ğ’‘)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end

"""
    FocalCELoss(x::Variable,
                label::AbstractArray;
                focus::Real=1.0f0,
                reduction::String="sum")

focal loss version CrossEntropyLoss
"""
function FocalCELoss(x::Variable{T},
                     label::AbstractArray;
                     focus::Real=1.0f0,
                     reduction::String="sum") where T
    return Loss(FocalCE(x, label, focus=focus), reduction=reduction)
end


"""
    FocalBCELoss(x::Variable,
                 label::AbstractArray;
                 focus::Real=1.0f0,
                 alpha::Real=0.5f0,
                 reduction::String="sum")

focal loss version BinaryCrossEntropyLoss
"""
function FocalBCELoss(x::Variable{T},
                      label::AbstractArray;
                      focus::Real=1.0f0,
                      alpha::Real=0.5f0,
                      reduction::String="sum") where T
    return Loss(FocalBCE(x, label, focus=focus, alpha=alpha), reduction=reduction)
end


"""
    NLogCrossEntropy(p::Variable, label::AbstractArray)
Loss = [ âˆ’ ln(`p`) ] * [ âˆ’ `label` * ln(`p`) ], where `p` is the predicted probability
"""
function NLogCrossEntropy(p::Variable{T}, label::AbstractArray) where T
    # Loss = (-ğ’ğ’ğ’‘)*(-ğœ¸ * ğ’ğ’ğ’‘), negative log weighted CELoss
    @assert p.shape == size(label)
    S = eltype(p)
    Ïµ = S(1e-38)
    ğœ¸ = label
    ğ’‘ = áµ›(p) .+ Ïµ
    ğ’ğ’ğ’‘ = log.(ğ’‘)
    y = Variable{T}(ğœ¸ .* ğ’ğ’ğ’‘ .* ğ’ğ’ğ’‘, p.backprop)
    if y.backprop
        ğŸš = S(2f0)
        y.backward = function âˆ‡NLogCrossEntropy()
            if need2computeÎ´!(p)
                Î´(p) .+= Î´(y) .* ğŸš .* ğœ¸ .* ğ’ğ’ğ’‘ ./ ğ’‘
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


function NLogCELoss(p::Variable, label::AbstractArray; reduction::String="sum")
    return Loss(NLogCrossEntropy(p, label), reduction=reduction)
end


"""
    InvPowerCrossEntropy(p::Variable{T}, label::AbstractArray; a::Real=0.3f0, n::Real=1f0)
Loss = [ 1 / (`p` + (1-`a`))`â¿` ] * [ âˆ’ `label` * ln(`p`) ], where `p` is the predicted probability.
`a` in [0, 0.5] is recommended.
"""
function InvPowerCrossEntropy(p::Variable{T}, label::AbstractArray; a::Real=0.3f0, n::Real=1f0) where T
    @assert p.shape == size(label)
    S = eltype(p)
    Ïµ = S(1e-38)
    a = S(1 - a)
    ğ’ = S(n)

    ğœ¸ = label
    ğ’‘ = áµ›(p) .+ Ïµ
    ğ’ğ’ğ’‘  = log.(ğ’‘)
    Q    = ğ’‘ .+ a
    Qâ¿   = Q .^ ğ’
    Qâ¿âºÂ¹ = Q .* Qâ¿
    y = Variable{T}( - ğœ¸ .* ğ’ğ’ğ’‘ ./ Qâ¿ , p.backprop)

    if y.backprop
        y.backward = function âˆ‡InvPowerCrossEntropy()
            if need2computeÎ´!(p)
                Î´y = Î´(y)
                Î´p = Î´(p)
                @. Î´p .+= Î´y * ğœ¸ * (ğ’ * ğ’‘ * ğ’ğ’ğ’‘ - Q) / (ğ’‘ * Qâ¿âºÂ¹)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, p)
    end
    return y
end


function InvPowerCELoss(p::Variable,
                        label::AbstractArray;
                        reduction::String="sum",
                        a::Real=0.3f0,
                        n::Real=1.0f0)
    return Loss(InvPowerCrossEntropy(p, label, a=a, n=n), reduction=reduction)
end
