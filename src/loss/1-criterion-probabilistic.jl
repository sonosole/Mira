## probabilistic loss

export crossEntropy
export crossEntropyLoss

export binaryCrossEntropy
export binaryCrossEntropyLoss


"""
    crossEntropy(x::Variable{T}, label::Variable{T}) -> Variable{T}
cross entropy = - y * log(Ì‚y) where y is target and Ì‚y is the output of the network.
"""
function crossEntropy(x::Variable{T}, label::Variable{T}) where T
    @assert (x.shape == label.shape)
    backprop = (x.backprop || label.backprop)
    Ïµ = eltype(x)(1e-38)
    y = Variable{T}(- áµ›(label) .* log.(áµ›(x) .+ Ïµ), backprop)
    if backprop
        y.backward = function crossEntropyBackward()
            if need2computeÎ´!(x)
                Î´(x) .-= Î´(y) .* áµ›(label) ./ (áµ›(x) .+ Ïµ)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, x)
    end
    return y
end

"""
    crossEntropy(x::Variable{T}, label::T) -> Variable{T}
cross entropy = - y * log(Ì‚y) where y is target and Ì‚y is the output of the network.
"""
function crossEntropy(x::Variable{T}, label::T) where T
    @assert x.shape == size(label)
    Ïµ = eltype(x)(1e-38)
    y = Variable{T}(- label .* log.(áµ›(x) .+ Ïµ), x.backprop)
    if y.backprop
        y.backward = function crossEntropyBackward()
            if need2computeÎ´!(x)
                Î´(x) .-= Î´(y) .* label ./ (áµ›(x) .+ Ïµ)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, x)
    end
    return y
end


crossEntropyLoss(x::Variable{T}, label::Variable{T}; reduction::String="sum") where T = loss( crossEntropy(x, label), reduction=reduction )
crossEntropyLoss(x::Variable{T}, label::T; reduction::String="sum") where T = loss( crossEntropy(x, label), reduction=reduction )


"""
    binaryCrossEntropy(x::Variable{T}, l::Variable{T}) -> Variable{T}
binary cross entropy = - y * log(Ì‚y) - (1 - y) * log(1-Ì‚y)
"""
function binaryCrossEntropy(x::Variable{T}, label::Variable{T}) where T
    @assert (x.shape == label.shape)
    backprop = (x.backprop || label.backprop)
    TOO  = eltype(x)
    Ïµ  = TOO(1e-38)
    ğŸ™  = TOO(1.0f0)
    tmp1 = - áµ›(label) .* log.(áµ›(x) .+ Ïµ)
    tmp2 = - (ğŸ™ .- áµ›(label)) .* log.(ğŸ™ .- áµ›(x) .+ Ïµ)
    y  = Variable{T}(tmp1 + tmp2, backprop)
    if backprop
        y.backward = function binaryCrossEntropyBackward()
            if need2computeÎ´!(x)
                temp1 = (ğŸ™ .- áµ›(label)) ./ (ğŸ™ .- áµ›(x) .+ Ïµ)
                temp2 = áµ›(label) ./ (áµ›(x) .+ Ïµ)
                Î´(x) .+= Î´(y) .* (temp1 - temp2)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y);
        end
        addchild(y, x)
    end
    return y
end


"""
    binaryCrossEntropy(x::Variable{T}, l::T) -> Variable{T}
binary cross entropy = - y * log(Ì‚y) - (1 - y) * log(1-Ì‚y)
"""
function binaryCrossEntropy(x::Variable{T}, label::T) where T
    @assert x.shape == size(label.shape)
    TO = eltype(x)
    Ïµ  = TO(1e-38)
    ğŸ™  = TO(1.0f0)
    tmp1 = - label .* log.(áµ›(x) .+ Ïµ)
    tmp2 = - (ğŸ™ .- label) .* log.(ğŸ™ .- áµ›(x) .+ Ïµ)
    y  = Variable{T}(tmp1 + tmp2, x.backprop)
    if y.backprop
        y.backward = function binaryCrossEntropyBackward()
            if need2computeÎ´!(x)
                temp1 = (ğŸ™ .- label) ./ (ğŸ™ .- áµ›(x) .+ Ïµ)
                temp2 = label ./ (áµ›(x) .+ Ïµ)
                Î´(x) .+= Î´(y) .* (temp1 - temp2)
            end
            ifNotKeepÎ´ThenFreeÎ´!(y)
        end
        addchild(y, x)
    end
    return y
end


binaryCrossEntropyLoss(x::Variable{T}, label::Variable{T}; reduction::String="sum") where T = loss( binaryCrossEntropy(x, label), reduction=reduction)
binaryCrossEntropyLoss(x::Variable{T}, label::T; reduction::String="sum") where T = loss( binaryCrossEntropy(x, label), reduction=reduction)
